{"cells":[{"cell_type":"markdown","metadata":{"id":"wNG4Hro1EHE4"},"source":["STEP 3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":56},"id":"KdRSp3IgA-K9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Upload 15 Financial Institution CSV files...\n"]},{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-caa95f67-3d03-42e4-8698-6d098d42b07e\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-caa95f67-3d03-42e4-8698-6d098d42b07e\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# Upload 15 CSVs for Financial Institutions\n","from google.colab import files\n","import pandas as pd\n","import io\n","\n","print(\"Upload 15 Financial Institution CSV files...\")\n","financial_uploads = files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYbn6TFoCDz6"},"outputs":[],"source":["# Process the Financial Institution CSVs\n","financial_data = {}\n","\n","for filename in financial_uploads:\n","    try:\n","        df = pd.read_csv(io.BytesIO(financial_uploads[filename]))\n","        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","        df = df.sort_values('Date')\n","        df.set_index('Date', inplace=True)\n","\n","        df['Price'] = df['Price'].astype(str).str.replace(',', '').str.replace('-', '')\n","        df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n","        df = df[['Price']].dropna()\n","\n","        ticker = filename.replace('.csv', '').strip()\n","        financial_data[ticker] = df['Price']\n","    except Exception as e:\n","        print(f\"Error loading {filename}: {e}\")\n","\n","financial_prices = pd.DataFrame(financial_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrmMGP_sCDvw"},"outputs":[],"source":["# Upload 15 CSVs for Non-Financial Institutions\n","print(\"Upload 15 Non-Financial Institution CSV files...\")\n","non_financial_uploads = files.upload()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z9rhyd6XCDs7"},"outputs":[],"source":["# Process the Non-Financial Institution CSVs\n","non_financial_data = {}\n","\n","for filename in non_financial_uploads:\n","    try:\n","        df = pd.read_csv(io.BytesIO(non_financial_uploads[filename]))\n","        df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n","        df = df.sort_values('Date')\n","        df.set_index('Date', inplace=True)\n","\n","        df['Price'] = df['Price'].astype(str).str.replace(',', '').str.replace('-', '')\n","        df['Price'] = pd.to_numeric(df['Price'], errors='coerce')\n","        df = df[['Price']].dropna()\n","\n","        ticker = filename.replace('.csv', '').strip()\n","        non_financial_data[ticker] = df['Price']\n","    except Exception as e:\n","        print(f\"Error loading {filename}: {e}\")\n","\n","non_financial_prices = pd.DataFrame(non_financial_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYSn86yECDqs"},"outputs":[],"source":["# Combine all price data\n","all_prices = pd.concat([financial_prices, non_financial_prices], axis=1)\n","all_prices_clean = all_prices.dropna()\n","\n","print(\"First 5 rows of combined price data:\")\n","print(all_prices_clean.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XL31LT9FCDnc"},"outputs":[],"source":["# Compute daily returns\n","daily_returns = all_prices_clean.pct_change().dropna()\n","\n","print(\"\\nFirst 5 rows of daily returns:\")\n","print(daily_returns.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ydmu40EmCDkh"},"outputs":[],"source":["# Plot daily returns for all 30 institutions\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(14, 7))\n","daily_returns.plot(figsize=(14, 7), linewidth=1, alpha=0.7)\n","plt.title('Daily Returns of Financial and Non-Financial Institutions (Sep–Oct 2008)')\n","plt.xlabel('Date')\n","plt.ylabel('Daily Return')\n","plt.legend(loc='upper right', fontsize='small', ncol=2)\n","plt.grid(True)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"5fbyLQX_JLSB"},"source":["STEP 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTIR3k7E8nK5"},"outputs":[],"source":["# Compute the 30x30 correlation matrix\n","correlation_matrix = daily_returns.corr()\n","print(\"30x30 Correlation Matrix:\")\n","print(correlation_matrix)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDs2WM2J8nId"},"outputs":[],"source":["# Produce the heatmap for the correlation matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(14, 12))\n","sns.heatmap(correlation_matrix, cmap=\"coolwarm\", annot=True, fmt=\".2f\", linewidths=0.5)\n","plt.title(\"Heatmap of 30x30 Correlation Matrix\", fontsize=16)\n","plt.xticks(rotation=90)\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3whyVmNM8nFs"},"outputs":[],"source":["# Clustered heatmap to group similar correlations\n","# Hierarchical clustering logic\n","sns.clustermap(\n","    correlation_matrix,\n","    cmap=\"coolwarm\",\n","    annot=True,\n","    fmt=\".2f\",\n","    linewidths=0.5,\n","    figsize=(15, 15),\n","    xticklabels=True,\n","    yticklabels=True\n",")\n","\n","plt.suptitle(\"Clustered Heatmap of Correlation Matrix\", y=1.02, fontsize=16)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"f1EAn508O9w2"},"source":["STEP 6"]},{"cell_type":"markdown","metadata":{"id":"EoWytTY0O9cM"},"source":["Team member A"]},{"cell_type":"markdown","metadata":{"id":"CDt9Oz9iMbr5"},"source":["\n","\n","Pseudocode: Upper Confidence Bound (UCB) Algorithm for k-Armed Bandit\n","Inputs:\n","\n","k: number of arms (actions)\n","\n","T: total number of time steps\n","\n","c: confidence level parameter (controls exploration)\n","\n","μ: true reward means (used for simulation)\n","\n","σ: standard deviation of reward noise (assumed 1 here)\n","\n","Initialize:\n","\n","For each action a in {1, 2, ..., k}:\n","\n","Set Q[a] ← 0  # estimated value (average reward)\n","\n","Set N[a] ← 0  # count of times action a was chosen\n","\n","For t from 1 to T:\n","\n","a. For each action a in {1, 2, ..., k}:\n","\n","If N[a] == 0:\n","\n","Set UCB[a] ← ∞  # force at least one exploration of each action\n","\n","Else:\n","\n","Set UCB[a] ← Q[a] + c * sqrt(ln(t) / N[a])\n","\n","b. Choose action A ← argmax_a UCB[a]\n","\n","c. Simulate reward:\n","\n","Draw R from normal distribution with mean μ[A] and standard deviation σ\n","\n","d. Update estimates:\n","\n","N[A] ← N[A] + 1\n","\n","Q[A] ← Q[A] + (1 / N[A]) * (R - Q[A])  # incremental average\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ls5gdvz_O3uG"},"source":["TEAM MEMBER B"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uxKlYv2uFQK8"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","# Parameters\n","num_bandits = 10         # Number of arms\n","num_steps = 1000         # Total time steps\n","true_means = np.random.normal(0, 1, num_bandits)  # True reward means for each arm\n","reward_std = 1.0         # Standard deviation of rewards\n","confidence_level = 2     # Exploration parameter 'c' in UCB\n","\n","# Tracking variables\n","Q_values = np.zeros(num_bandits)    # Estimated values (Q)\n","action_counts = np.zeros(num_bandits)  # Count of times each arm was pulled\n","rewards = np.zeros(num_steps)       # Reward at each time step\n","optimal_action = np.argmax(true_means)\n","\n","# UCB algorithm\n","for t in range(1, num_steps + 1):\n","    ucb_values = np.zeros(num_bandits)\n","\n","    for i in range(num_bandits):\n","        if action_counts[i] == 0:\n","            ucb_values[i] = float('inf')  # Force exploration of each arm once\n","        else:\n","            bonus = confidence_level * np.sqrt(np.log(t) / action_counts[i])\n","            ucb_values[i] = Q_values[i] + bonus\n","\n","    # Select action with highest UCB\n","    chosen_action = np.argmax(ucb_values)\n","\n","    # Simulate reward from chosen arm\n","    reward = np.random.normal(loc=true_means[chosen_action], scale=reward_std)\n","\n","    # Update counts and estimated Q-value\n","    action_counts[chosen_action] += 1\n","    Q_values[chosen_action] += (reward - Q_values[chosen_action]) / action_counts[chosen_action]\n","\n","    # Store reward\n","    rewards[t - 1] = reward\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMxNOycDFQHl"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Set seed for reproducibility\n","np.random.seed(42)\n","\n","# Prepare the environment\n","stocks = daily_returns.columns.tolist()        # 30 stocks\n","num_bandits = len(stocks)                      # Number of arms (30)\n","num_steps = len(daily_returns)                 # Each time step is a trading day\n","\n","# Use the actual return matrix as the reward source\n","return_matrix = daily_returns[stocks].values\n","\n","# Initialize parameters\n","Q_values = np.zeros(num_bandits)\n","action_counts = np.zeros(num_bandits)\n","rewards = np.zeros(num_steps)\n","chosen_arms = []\n","\n","confidence_level = 2  # UCB 'c' parameter\n","\n","# UCB loop across time steps (i.e., trading days)\n","for t in range(1, num_steps + 1):\n","    ucb_values = np.zeros(num_bandits)\n","\n","    for i in range(num_bandits):\n","        if action_counts[i] == 0:\n","            ucb_values[i] = float('inf')  # Force initial exploration\n","        else:\n","            bonus = confidence_level * np.sqrt(np.log(t) / action_counts[i])\n","            ucb_values[i] = Q_values[i] + bonus\n","\n","    # Choose the arm with the highest UCB\n","    chosen_action = np.argmax(ucb_values)\n","    chosen_arms.append(stocks[chosen_action])\n","\n","    # Use actual return on that day for the chosen stock as reward\n","    reward = return_matrix[t - 1, chosen_action]\n","\n","    # Update counts and Q-values\n","    action_counts[chosen_action] += 1\n","    Q_values[chosen_action] += (reward - Q_values[chosen_action]) / action_counts[chosen_action]\n","    rewards[t - 1] = reward\n","\n","# Plot average reward over time\n","average_rewards = np.cumsum(rewards) / (np.arange(num_steps) + 1)\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(average_rewards, label='Average Reward (UCB)', color='blue')\n","plt.xlabel('Day (Time Step)')\n","plt.ylabel('Average Daily Return')\n","plt.title('UCB Algorithm Applied to 30 Stocks')\n","plt.grid(True)\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PVFKFC0FFQEs"},"outputs":[],"source":["import pandas as pd\n","\n","# Count how many times each stock was selected\n","selection_counts = pd.Series(chosen_arms).value_counts().sort_values(ascending=False)\n","\n","# Plot bar chart\n","plt.figure(figsize=(12, 6))\n","selection_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n","plt.title(\"Frequency of Each Stock Selected by UCB Algorithm\", fontsize=14)\n","plt.xlabel(\"Stock\")\n","plt.ylabel(\"Number of Selections\")\n","plt.xticks(rotation=45)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoVgLtKmR-y6"},"outputs":[],"source":["# Create a DataFrame to store the selection counts and corresponding average return\n","average_returns = [np.mean(return_matrix[:, i][np.array(chosen_arms) == stocks[i]]) for i in range(num_bandits)]\n","\n","# Create a DataFrame for easy viewing\n","stock_performance = pd.DataFrame({\n","    'Stock': stocks,\n","    'Selections': selection_counts.reindex(stocks).fillna(0).astype(int),  # Fixing the reindex and filling missing values with 0\n","    'Average Return': average_returns\n","})\n","\n","# Sort by number of selections (descending) and then by average return (descending)\n","top_stocks = stock_performance.sort_values(by=['Selections', 'Average Return'], ascending=False).head(5)\n","\n","# Display the top 5 stocks with their selections and average returns\n","top_stocks = top_stocks[['Stock', 'Selections', 'Average Return']]\n","top_stocks\n"]},{"cell_type":"markdown","metadata":{"id":"DyZGCnD2PkKD"},"source":["STEP 8"]},{"cell_type":"markdown","metadata":{"id":"d0bVJVMKSt-R"},"source":["Team member B"]},{"cell_type":"markdown","metadata":{"id":"vaTRQIRVS1iW"},"source":["Pseudocode: ε-Greedy Algorithm for Non-Stationary k-Armed Bandits\n","Initialize:\n","Set number of bandits k\n","\n","Set number of time steps T\n","\n","Set exploration rate ε (epsilon)\n","\n","Set step size α (for constant step-size version)\n","\n","For each bandit i in 1 to k:\n","\n","Initialize estimated value Q[i] ← 0\n","\n","Initialize action count N[i] ← 0\n","\n","For each episode:\n","Reset Q and N to zeros\n","\n","Set current regime r ← 0\n","\n","For each time step t = 1 to T:\n","If regime changes at time t, update current regime r\n","\n","Get true reward means μ[1...k] for regime r\n","\n","Choose action a using ε-greedy:\n","\n","With probability ε, choose a random action\n","\n","With probability 1 - ε, choose action a with highest Q[a]\n","\n","If multiple actions tie, select randomly among them\n","\n","Observe reward R by sampling from N(μ[a], 1)\n","\n","Increment action count: N[a] ← N[a] + 1\n","\n","Update Q-value:\n","\n","If using sample averaging:\n","Q[a] ← Q[a] + (1 / N[a]) * (R - Q[a])\n","\n","If using constant step-size:\n","Q[a] ← Q[a] + α * (R - Q[a])\n","\n","(Optional) Track statistics like average reward and % optimal action\n","\n","End For (time step loop)\n","Repeat for all episodes to average performance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FMsZ10DBVyAp"},"outputs":[],"source":["from graphviz import Digraph\n","\n","# Create a new directed graph\n","dot = Digraph(comment='Epsilon-Greedy Algorithm Flowchart')\n","\n","# Nodes\n","dot.node('A', 'Start')\n","dot.node('B', 'Initialize:\\nQ[a] = 0\\nN[a] = 0\\nε, α, T')\n","dot.node('C', 'For each time step t = 1 to T')\n","dot.node('D', 'Generate random number\\nr ∈ [0, 1]')\n","dot.node('E', 'r \u003c ε?')\n","dot.node('F', 'Choose random action a_t')\n","dot.node('G', 'Choose greedy action:\\na_t = argmax Q[a]')\n","dot.node('H', 'Observe reward r_t\\nfrom action a_t')\n","dot.node('I', 'Increment count:\\nN[a_t] += 1')\n","dot.node('J', 'Update estimate:\\nQ[a_t] ← Q[a_t] + α * (r_t - Q[a_t])')\n","dot.node('K', 't \u003c T?')\n","dot.node('L', 'Next time step')\n","dot.node('M', 'End')\n","\n","# Edges\n","dot.edge('A', 'B')\n","dot.edge('B', 'C')\n","dot.edge('C', 'D')\n","dot.edge('D', 'E')\n","dot.edge('E', 'F', label='Yes')\n","dot.edge('E', 'G', label='No')\n","dot.edge('F', 'H')\n","dot.edge('G', 'H')\n","dot.edge('H', 'I')\n","dot.edge('I', 'J')\n","dot.edge('J', 'K')\n","dot.edge('K', 'L', label='Yes')\n","dot.edge('L', 'C')\n","dot.edge('K', 'M', label='No')\n","\n","# Save or render\n","dot.render('epsilon_greedy_flowchart', format='png', cleanup=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi_gJ5pfFP_A"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"zPvBC7dcSwaj"},"source":["TEAM MEMEBR C"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0mz0wLzYNCU"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","\n","# ---------------------------\n","# Parameters\n","# ---------------------------\n","NK = 10                    # Number of bandits (we'll use the first 10 stocks)\n","ITEMAX = 1000              # Time steps (for this simulation)\n","NEPISODES = 1000           # Runs for averaging\n","EPSILON = 0.1              # ε for exploration\n","ALPHA = 0.2                # Constant step-size for non-stationary update\n","\n","# ---------------------------\n","# Prepare Data (from your dataset)\n","# ---------------------------\n","# Use first 10 stocks for simplicity\n","stocks = daily_returns.columns[:NK]  # First 10 columns (stocks)\n","returns = daily_returns[stocks]  # Subset of returns\n","\n","# ---------------------------\n","# ε-greedy Selection Function\n","# ---------------------------\n","def select_action(qvalue, epsilon):\n","    \"\"\"\n","    ε-greedy action selection.\n","    With probability ε: random action.\n","    With probability 1 - ε: greedy action.\n","    \"\"\"\n","    if np.random.rand() \u003c epsilon:\n","        return np.random.randint(len(qvalue))  # Explore\n","    else:\n","        max_q = np.max(qvalue)\n","        best_actions = np.where(qvalue == max_q)[0]\n","        return np.random.choice(best_actions)  # Exploit with tie-breaking\n","\n","# ---------------------------\n","# Reward Update Function\n","# ---------------------------\n","def update_qvalue(qvalue_old, action, reward, alpha):\n","    qvalue_new = qvalue_old.copy()\n","    qvalue_new[action] += alpha * (reward - qvalue_old[action])\n","    return qvalue_new\n","\n","# Ensure ITEMAX does not exceed the number of rows in `returns`\n","ITEMAX = len(returns)  # Set ITEMAX to the number of available time steps (rows)\n","\n","# ---------------------------\n","# Run Simulation\n","# ---------------------------\n","reward_avg = np.zeros((ITEMAX, 2))      # [averaging, constant-alpha]\n","optimal_avg = np.zeros((ITEMAX, 2))\n","\n","for update_type in range(2):  # 0: averaging, 1: constant-alpha\n","    for episode in range(NEPISODES):\n","        qvalue = np.zeros(NK)\n","        action_count = np.zeros(NK)\n","        regime_index = 0\n","        for t in range(ITEMAX):\n","            true_means = returns.iloc[t].values  # Get the returns for the current time step\n","\n","            # Choose action\n","            action = select_action(qvalue, EPSILON)\n","\n","            # Sample reward (daily return of the chosen stock)\n","            reward = returns.iloc[t, action]\n","\n","            # Update counts and Q-values\n","            action_count[action] += 1\n","            if update_type == 0:\n","                alpha = 1.0 / action_count[action]  # Sample averaging\n","            else:\n","                alpha = ALPHA                      # Constant step-size\n","\n","            qvalue = update_qvalue(qvalue, action, reward, alpha)\n","\n","            # Logging performance\n","            reward_avg[t, update_type] += reward / NEPISODES\n","            if action == np.argmax(true_means):\n","                optimal_avg[t, update_type] += 1.0 / NEPISODES\n","\n","# ---------------------------\n","# Plotting results\n","# ---------------------------\n","plt.figure(figsize=(10, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(reward_avg[:, 0], label='Sample Averaging')\n","plt.plot(reward_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Average Reward\")\n","plt.title(\"Reward over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(optimal_avg[:, 0], label='Sample Averaging')\n","plt.plot(optimal_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Optimal Action %\")\n","plt.title(\"Optimal Action Over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOcviKgNFP8k"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from numpy.random import rand, seed\n","\n","# ---------------------------\n","# Parameters\n","# ---------------------------\n","NK = 10                    # Number of bandits\n","ITEMAX = 1000              # Time steps\n","NEPISODES = 1000           # Runs for averaging\n","EPSILON = 0.1              # ε for exploration\n","ALPHA = 0.2                # Constant step-size for non-stationary update\n","EPSILON_M = [0.0, 0.1]     # For comparison (greedy vs ε-greedy)\n","\n","seed(1234)\n","\n","# ---------------------------\n","# Regime Change Points\n","# ---------------------------\n","PCHANGE = 0.01  # Probability of reward distribution change\n","tchanges = np.zeros(())\n","ntchanges = 0\n","for t in range(ITEMAX):\n","    if rand() \u003c PCHANGE:\n","        tchanges = np.append(tchanges, t)\n","        ntchanges += 1\n","\n","# Initialize true reward means for all regimes\n","NMEANS = np.random.normal(loc=0.0, scale=1.0, size=(NK, ntchanges + 1))\n","\n","# ---------------------------\n","# Plot reward structure across regimes\n","# ---------------------------\n","for bandit in range(NK):\n","    plt.plot(NMEANS[bandit, :])\n","plt.plot(np.max(NMEANS, axis=0), linewidth=3.0, label=\"Max Reward\")\n","plt.xlabel(\"Regime\")\n","plt.title(\"True Bandit Reward Means\")\n","plt.grid(True)\n","plt.show()\n","\n","# ---------------------------\n","# ε-greedy Selection Function\n","# ---------------------------\n","def select_action(qvalue, epsilon):\n","    \"\"\"\n","    ε-greedy action selection.\n","    With probability ε: random action.\n","    With probability 1 - ε: greedy action.\n","    \"\"\"\n","    if rand() \u003c epsilon:\n","        return np.random.randint(len(qvalue))  # Explore\n","    else:\n","        max_q = np.max(qvalue)\n","        best_actions = np.where(qvalue == max_q)[0]\n","        return np.random.choice(best_actions)  # Exploit with tie-breaking\n","\n","# ---------------------------\n","# Reward Update Function\n","# ---------------------------\n","def update_qvalue(qvalue_old, action, reward, alpha):\n","    qvalue_new = qvalue_old.copy()\n","    qvalue_new[action] += alpha * (reward - qvalue_old[action])\n","    return qvalue_new\n","\n","# ---------------------------\n","# Run Simulation\n","# ---------------------------\n","reward_avg = np.zeros((ITEMAX, 2))      # [averaging, constant-alpha]\n","optimal_avg = np.zeros((ITEMAX, 2))\n","\n","# Ensure the action is within the bounds of available stocks (columns in returns)\n","N_STOCKS = returns.shape[1]  # Number of available stocks (columns)\n","\n","# Ensure that the loop doesn't exceed the number of available rows in returns\n","n_rows = returns.shape[0]  # Number of rows in the returns DataFrame\n","\n","for update_type in range(2):  # 0: averaging, 1: constant-alpha\n","    for episode in range(NEPISODES):\n","        qvalue = np.zeros(NK)\n","        action_count = np.zeros(NK)\n","        regime_index = 0\n","        for t in range(min(ITEMAX, n_rows)):  # Ensure t doesn't exceed the number of rows\n","            true_means = returns.iloc[t].values  # Get the returns for the current time step\n","\n","            # Choose action, ensuring the action index is within bounds\n","            action = np.random.choice(range(N_STOCKS))  # Random action between 0 and N_STOCKS-1\n","\n","            # Sample reward (daily return of the chosen stock)\n","            reward = returns.iloc[t, action]\n","\n","            # Update counts and Q-values\n","            action_count[action] += 1\n","            if update_type == 0:\n","                alpha = 1.0 / action_count[action]  # Sample averaging\n","            else:\n","                alpha = ALPHA                      # Constant step-size\n","\n","            qvalue = update_qvalue(qvalue, action, reward, alpha)\n","\n","            # Logging performance\n","            reward_avg[t, update_type] += reward / NEPISODES\n","            if action == np.argmax(true_means):\n","                optimal_avg[t, update_type] += 1.0 / NEPISODES\n","\n","# ---------------------------\n","# Plotting results\n","# ---------------------------\n","plt.figure(figsize=(10, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(reward_avg[:, 0], label='Sample Averaging')\n","plt.plot(reward_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Average Reward\")\n","plt.title(\"Reward over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(optimal_avg[:, 0], label='Sample Averaging')\n","plt.plot(optimal_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Optimal Action %\")\n","plt.title(\"Optimal Action Over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"fc_YnXVcSzfy"},"source":["TEAM MEMBER A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Id5_zqdrSzG2"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from numpy.random import rand, seed\n","\n","# Assuming you already have 'all_prices_clean' and 'daily_returns' computed\n","\n","# ---------------------------\n","# Parameters\n","# ---------------------------\n","NK = len(daily_returns.columns)        # Number of bandits (stocks)\n","ITEMAX = len(daily_returns)            # Number of time steps (number of days)\n","NEPISODES = 1000                      # Runs for averaging\n","EPSILON = 0.1                         # ε for exploration\n","ALPHA = 0.2                           # Constant step-size for non-stationary update\n","EPSILON_M = [0.0, 0.1]                # For comparison (greedy vs ε-greedy)\n","\n","seed(1234)\n","\n","# ---------------------------\n","# ε-greedy Selection Function\n","# ---------------------------\n","def select_action(qvalue, epsilon):\n","    if rand() \u003c epsilon:\n","        return np.random.randint(len(qvalue))  # Explore\n","    else:\n","        max_q = np.max(qvalue)\n","        best_actions = np.where(qvalue == max_q)[0]\n","        return np.random.choice(best_actions)  # Exploit with tie-breaking\n","\n","# ---------------------------\n","# Reward Update Function\n","# ---------------------------\n","def update_qvalue(qvalue_old, action, reward, alpha):\n","    qvalue_new = qvalue_old.copy()\n","    qvalue_new[action] += alpha * (reward - qvalue_old[action])\n","    return qvalue_new\n","\n","# ---------------------------\n","# Run Simulation\n","# ---------------------------\n","reward_avg = np.zeros((ITEMAX, 2))      # [averaging, constant-alpha]\n","optimal_avg = np.zeros((ITEMAX, 2))\n","\n","for update_type in range(2):  # 0: averaging, 1: constant-alpha\n","    for episode in range(NEPISODES):\n","        qvalue = np.zeros(NK)          # Initial Q-values (estimates)\n","        action_count = np.zeros(NK)    # Count of actions taken for each bandit\n","        for t in range(ITEMAX):\n","            # Select action based on current Q-values and epsilon\n","            action = select_action(qvalue, EPSILON)\n","\n","            # Sample reward (daily return of the chosen stock)\n","            reward = daily_returns.iloc[t, action]\n","\n","            # Update counts and Q-values\n","            action_count[action] += 1\n","            if update_type == 0:\n","                alpha = 1.0 / action_count[action]  # Sample averaging\n","            else:\n","                alpha = ALPHA                      # Constant step-size\n","\n","            qvalue = update_qvalue(qvalue, action, reward, alpha)\n","\n","            # Logging performance\n","            reward_avg[t, update_type] += reward / NEPISODES\n","            if action == np.argmax(daily_returns.iloc[t, :]):\n","                optimal_avg[t, update_type] += 1.0 / NEPISODES\n","\n","# ---------------------------\n","# Plotting results\n","# ---------------------------\n","plt.figure(figsize=(10, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(reward_avg[:, 0], label='Sample Averaging')\n","plt.plot(reward_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Average Reward\")\n","plt.title(\"Reward over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(optimal_avg[:, 0], label='Sample Averaging')\n","plt.plot(optimal_avg[:, 1], label='Constant Step-Size')\n","plt.xlabel(\"Steps\")\n","plt.ylabel(\"Optimal Action %\")\n","plt.title(\"Optimal Action Over Time\")\n","plt.legend()\n","plt.grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OIvLOoVsSzC4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uaYnX_9OSzAZ"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFvdYpqfSy9j"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tK_-zGwRSy7G"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JfG9qjplSy4m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HqgRbGMDSy19"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXZCfUGLSyy6"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}